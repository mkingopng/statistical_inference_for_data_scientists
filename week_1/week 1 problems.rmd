# Week 1 Problems

## Contents
### Elements of Probability
- the tennis game
- the birthday problem
- conditional probabilities

### Random variables and univariate distributions

### multivariate distributions

### The General Inference Problem

```{r}

```


## The Tennis game
Q4: suppose there are 8 players, find the probability that they meet in the first or second round (2dp). You might want to use R below to calculate this probability
```{r}
players <- 8  # why doesn't this get used?
n <- 3  # the number of rounds. 8 players mean 3 rounds
n/(2 * (2^3 - 1))
```
This is kinda clunky. I'm sure there is a more elegant way.

## The birthday problem
### The prod() function
Q1: calculate the probability that at least two people have the same birthday when n people are gathered in a room
```{r}
1 - prod(365:(365-n+1))/365^n
```

Q2:
calculate the probability that at least 2 people have the same birthday for n=22

Hint: use R below and the function prod() which returns the multiplication results of all the values present in its arguments

```{r}
n <- 22  # the number of people is 22
1 - prod(365:(365-n+1))/365^n
```

### Q3:
calculate the probability that at least two people have the same birthday for n=23 (2dp)

Hint: use R below and the function prod() which returns the multiplication results of all the values present in its arguments

```{r}
n <- 23  # the number of people is 23
1 - prod(365:(365-n+1))/365^n
```

**Q4**: find the probability that at least one person has the same birthday as you and find the value of n so that this probability approaches $\dfrac{1}{2}$

**solution**: the number of outcomes not in the event is $364^n$ and hence the probability that one person has the same birthday as yuo is

$$1 - (364/365)^n$$

we want it to be near $\dfrac{1}{2}$ so

$$ \Bigg(\dfrac{364}{365}\Bigg)^n \approx \dfrac{1}{2}$$

and by taking the natural log we get

$$ n \approx \dfrac{\log(0.5)}{\log(364/365)} \approx 252.65$$

```{r}
p <- 0.5
not <- 364
doy <- 365
log(p)/log(not/doy)
```
## conditional probabilities and independence

```{r}
## For i = 1
1/choose(52-1,4-1)
## For i = 2
1/choose(52-2,4-2)
## For i = 3
1/choose(52-3,4-3)
```
**Q**
two litters of a particular rodent species have been born, one with two-brown-haired and one grey-haired (litter 1, 3), and the other with three brown-haired and two grey=haired (litter 2, 5). We select a litter at random and then select offspring at random from the selected litter.

```{r}
l1 <- 2/3  # litter_1
l2 <- 3/5  # litter_2

probability <- (l1 + l2)/2
probability
```
**Q**

given that a brown-haired offspring was selected, find the probability that the sampling was from litter 1

comment: use bayes theorem

$$
\begin{aligned}
P(\text{Litter 1| Brown Hair}) &= \dfrac{P(BH|L1)P(L1)}{P(BH|L!)P(L1)+P(BH|L2)P(L2)} \\
&= \dfrac{\dfrac{2}{3} \times \dfrac{1}{2}}{\dfrac{19}{30}} \\
&= \dfrac{10}{19} \\
&= 0.53 \\
\end{aligned}
$$

```{r}
proba <- (2/3 * 1/2)/(2/3 * 1/2 + 3/5 * 1/2)
proba
```

# Random Variables & Univariate Distributions

**Q**

```{r}

library("ggplot2")
library(ggplot2)

headstails <- data.frame(s=c("HHH","HHT","HTH","THH","TTH","THT","HTT","TTT"),
                         X=c(3,2,2,2,1,1,1,0))

ggplot(data = headstails, aes(x = X, y = ..density..)) +
  geom_histogram(bins=4,  fill="royalblue", col="grey") +
  scale_y_continuous(breaks = seq(0, 1, by = 0.05))
```

**Q**

```{r}
library(ggplot2)

x <- 1:8
p <- 0.6
P <- (1-p)^(x-1)*p
dat <- data.frame(x,P)
ggplot(data=dat, aes(x=x, y=P)) +
  geom_bar(stat="identity", width=0.1,fill="orange2") +
  scale_x_continuous(breaks = seq(0, 10, by = 1))
```

Suppose that on average 5 customers walk into a store every minute

**Q1**
find the probability that 10 customers come into the store in the next minute (3dp)

**Hint**: try the function ```dpois(x, lambda)``` which calculates $P(X=x)$ when $X \sim \text{Poisson}(\lambda)$
```{r}

```
**Q2**
Find the probability that at least five customer come into the store in the next minute (3dp)

**Hint**: try the function ```ppois(x, lambda)``` which calculates $P(X \leq x)$ when $X \sim \atext{Poisson}(\lambda)$
```{r}

```

## The binomial distribution

Suppose there are 10 multiple choice questions in a math quiz, where each question has four possible answers and only one of them is correct

**Hint**: Find the value of p first

**Q** Find the probability of having exactly 5 correct answers

**Hint**: Try the function dbnom(x,n,p) which calculatges $P(X=x)$ when $X \sim \text{Binomial}(n,p)$

```{r}
dbinom(x=5, size=10, prob=0.25)
```
## cumulative distribution function

```{r}
P <- ecdf(headstails$X)

P(0)
#[1] 0.125
P(1)
#[1] 0.5
P(2)
#[1] 0.875
P(3)
#[1] 1

ggplot(headstails , aes(x=X)) + stat_ecdf(geom = "step", colour='royalblue')

```
## cumulative distribution functions


```{r}
# sample size
n <- 1000

# random geometric data
x.geom <- rgeom(n=n, prob=.3)

# Cumulative Distribution Function
plot(ecdf(x.geom),
     xlab = "x",
     ylab = "P(X <= x)",
     col = "royalblue",
     cex = 0.3,
     lwd = 0.2,
     pch = 21,
     main = "Cumulative Distribution Function \n Geometric(n=1000, p=0.3)")

```
## probability density function theorem

Determine the value of c that makes f(x) a pdf (2dp)
```{r}

```
## common continuous distributions

```{r}
library(ggplot2)
x <- seq(-7, 7, by = 1/8)		# Range of x
y1 <- dt(x, df = 2)				# t distribution for 2 degrees of freedom
y2 <- dt(x, df = 4)				# t distribution for 4 degrees of freedom
y3 <- dt(x, df = 8)				# t distribution for 8 degrees of freedom
y4 <- dt(x, df = 16)			# t distribution for 16 degrees of freedom
y5 <- dt(x, df = 32)			# t distribution for 32 degrees of freedom
plot(x, y1,type="l",xlab="X",ylab="t distribution", ylim=range(seq(0,0.4,by=0.1)),col="blue")
lines(x, y2,type="l", col="red")
lines(x, y3,type="l", col="cyan")
lines(x, y4,type="l", col="green")
lines(x, y5,type="l", col="yellow")
legend("topright", legend=c("v=2","v=4", "v=8", "v=16", "v=32"),
       col=c("blue","red", "cyan", "green", "yellow"), lty=1)
```



```{r}
library(dplyr)
library(ggplot2)
library(tidyr)

data.frame(chisq = seq(0, 25, by=0.01)) %>%
  mutate(df_2 = dchisq(x = chisq, df = 2),
         df_4 = dchisq(x = chisq, df = 4),
         df_8 = dchisq(x = chisq, df = 8)) %>%
  gather(key = "df", value = "density", -chisq) %>%
  ggplot() +
  geom_line(aes(x = chisq, y = density, color = df)) +
  labs(title = "Chi-Square distribution for various Degrees of Freedom",
       x = "x",
       y = "pdf")
```

## transformations and expectations


```{r}
library(ggplot2)

base <- ggplot(data.frame(x = c(0,1)), aes(x))
myfunc <- function(x) -log(x)
base + stat_function(fun = myfunc, colour = "blue") +
  scale_x_continuous(breaks = seq(0, 1, by = 0.2)) +
  ylim(0,10)

```

## plotting pdfs

```{r}
## Plot the density function
pdf <- function(y) 14*y*(1-y^(1/3))
curve(pdf,0,1)
## Show that the density function integrates to one
integrate(pdf,0,1)
```


```{r}
## Plot the density function
pdf = function(y) (7/4)*exp(-(7/4)*(y-3))
curve(pdf,3,10)
## Show that the density function integrates to one
integrate(pdf,3,Inf)
```


**Q**

```{r}

```

# the exponential density

```{r}
data <- -log(1-runif(10000))
hist(data, prob = TRUE, breaks = 100)
curve(exp(-x), add = TRUE, col = "purple", lwd = 2)
```

# R's built-in distributions

```{r}
n <- 10000
alpha <- 0.8
beta <- 5
sim.dat <- rgamma(n, alpha, beta)
hist(sim.dat, prob = TRUE, breaks = 100)
curve(dgamma(x,alpha,beta),add=TRUE,col="orange", lwd = 2)
```

## Live webcast estimation

**Q1**

```{r}

```

**Q2**

```{r}

```

## moments

**Q**
```{r}

```

## exponential pdf

```{r}

```

## exponential cdf

```{r}

```


## exponential expectation

```{r}

```

## Exponential Variance

```{r}

```

# multivariate distributions

```{r}

```

## pdf

**Q1**: find the value of $C$

```{r}

```

**Q2**: find the joint cdf of $X$ and $Y$

```{r}

```

**Q3**: compute the probability $P(X \leq 1, Y \leq 0.25)$ to 2dp

```{r}

```

## pmf of two independent random variables

**Q1**: what is the joint pmf of $(X,Y)$?

**Hint**: the joint pmf of independent random variables is given by $P(X=x,Y=y)=P(Y=y)$

```{r}

```
## marginal distributions
**Q**: consider the pdf

find the marginal distribution of $X$

```{r}

```

## pdf of transformation

```{r}

```

## transformation of joint distribution
```{r}

```

# the general inference problem

```{r}

```

# Case Study: Normal Distribution task

**Q1**: show that $\mathbb{E}(X)=\mu$ and $Var(X)$

```{r}

```

**Q2**: show that $X$ has the normal pdf with mean $\mu$ and variance $\sigma^2$.

**Hint**: You will need to apply the density transformation formula

```{r}

```



